# SouthStack â€” Developer Documentation

> Architecture reference for the SouthStack project. Keep this file updated as the codebase evolves.

---

## 1. Project Overview

SouthStack is an **in-browser AI-powered code editor** that lets users create and iterate on full-stack web projects entirely inside a browser tab. It combines a Monaco-style editor UI with a live **WebContainer** (a Node.js-compatible runtime running in a Service Worker) so that code changes are immediately executed and previewed without any server round-trip. The AI coding agent runs as a **Web Worker** powered by WebLLM (a fully client-side LLM runtime), meaning the language model never sends user code to an external API â€” all inference happens locally on the user's GPU/CPU.

**Core Tech Stack:** Next.js 16 (App Router) Â· React 19 Â· TypeScript Â· WebContainers API Â· WebLLM (in-browser LLM) Â· Web Crypto API Â· OPFS (Origin Private File System) Â· Tailwind CSS v4

---

## 2. Architecture Map

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Browser Tab                                  â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Main UI Thread     â”‚        â”‚        AI Web Worker             â”‚ â”‚
â”‚  â”‚  (Next.js / React)   â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚  (public/ai-worker.js)          â”‚ â”‚
â”‚  â”‚                      â”‚ postMsgâ”‚  â€¢ WebLLM model loaded here     â”‚ â”‚
â”‚  â”‚  â€¢ Editor UI         â”‚        â”‚  â€¢ Streams tokens back to UI    â”‚ â”‚
â”‚  â”‚  â€¢ Preview iframe    â”‚        â”‚  â€¢ No DOM access                â”‚ â”‚
â”‚  â”‚  â€¢ Auth / Sessions   â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚                      â”‚                                             â”‚
â”‚  â”‚        â”‚FS sync       â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚        â–¼              â”‚        â”‚      WebContainer Instance        â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚  (Service Worker / WASM)         â”‚ â”‚
â”‚  â”‚  â”‚  OPFS Layer   â”‚   â”‚  FS opsâ”‚  â€¢ Runs Node.js in-browser       â”‚ â”‚
â”‚  â”‚  â”‚  (Encrypted)  â”‚   â”‚        â”‚  â€¢ npm install / dev server      â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚        â”‚  â€¢ Exposes localhost preview URL  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Lifecycle of a Code Change: User Prompt â†’ WebContainer FS

```
 User types prompt
       â”‚
       â–¼
 ai-context.ts        Scrapes OPFS file tree + active file content
       â”‚               into a structured ProjectContext object
       â–¼
 ai-engine.ts         Packages context into a system prompt and
       â”‚               sends it to the AI Web Worker via postMessage
       â–¼
 ai-worker.js         Receives messages, runs WebLLM inference,
       â”‚               streams raw token chunks back to main thread
       â–¼
 ai-parser.ts         Parses streamed output into structured
       â”‚               FileAction objects (FILE: / PATCH: blocks)
       â–¼
 ChatSidebar.tsx       Displays a diff/preview of each FileAction
       â”‚               behind the Safety Gate (Keep / Discard)
       â–¼
 [User clicks Keep]
       â”‚
       â–¼
 opfs.ts / opfs-crypto.ts   Encrypts & persists the new file content
       â”‚                     to OPFS for durable storage
       â–¼
 wc-sync-manager.ts   Writes the accepted file content into
       â”‚               webcontainerInstance.fs (in-memory FS)
       â–¼
 WebContainer          Vite HMR detects the change, hot-reloads
                        the preview iframe automatically
```

---

## 3. File Directory Guide

### `app/` â€” Next.js App Router Pages & Components

| File | Responsibility |
|---|---|
| `app/layout.tsx` | Root layout; wraps the app in `AuthProvider`, sets global fonts and metadata |
| `app/page.tsx` | Public landing page; renders Hero, Features, HowItWorks, and Footer sections |
| `app/globals.css` | Global Tailwind base styles and CSS custom properties (brand tokens) |
| `app/dashboard/page.tsx` | Authenticated project list; shows saved projects and the New Project button |
| `app/editor/page.tsx` | Editor redirect/shell; forwards to a specific project route |
| `app/editor/[projectId]/page.tsx` | **Main editor page** â€” boots WebContainer, mounts all editor panels, owns top-level state |
| `app/login/page.tsx` | Login form; delegates credential validation to `lib/auth.ts` |
| `app/signup/page.tsx` | Sign-up form; creates user record and derives the per-user encryption key |
| `app/test-suite/page.tsx` | Internal developer page for smoke-testing WebContainer behaviour |
| `app/components/AuthGate.tsx` | HOC that redirects unauthenticated users away from protected routes |
| `app/components/AuthProvider.tsx` | React context provider that holds session state and exposes `useAuth()` |
| `app/components/EditorMockup.tsx` | Animated static mockup of the editor used on the landing page hero |
| `app/components/FeaturesGrid.tsx` | Marketing component rendering the feature card grid |
| `app/components/NewProjectModal.tsx` | Modal for naming and creating a new project; calls `lib/projects.ts` |
| `app/components/editor/ChatSidebar.tsx` | **AI chat panel** â€” streams tokens, buffers output, owns `pendingChanges` Safety Gate state |
| `app/components/editor/DiffView.tsx` | Renders a unified diff for a single FileAction with Accept/Reject buttons |
| `app/components/editor/MarkdownRenderer.tsx` | Rich markdown + syntax-highlighted code block renderer for chat messages |
| `app/components/editor/WebTerminal.tsx` | xterm.js terminal showing WebContainer stdout/stderr; has "Fix with AI" button |

### `lib/` â€” Core Business Logic & Utilities

| File | Responsibility |
|---|---|
| `lib/ai-context.ts` | Builds `ProjectContext` (file tree + active file) and `formatAgentPrompt()` for the LLM |
| `lib/ai-engine.ts` | Persistent AI engine singleton via React Context; owns the Web Worker ref and streaming callbacks |
| `lib/ai-parser.ts` | Parses raw LLM output into `FileAction[]` (FILE: full rewrites and PATCH: search-replace blocks) |
| `lib/auth.ts` | Server-side auth helpers: password hashing, credential verification, user record CRUD |
| `lib/crypto.ts` | Web Crypto API wrappers for AES-GCM key derivation, encryption, and decryption |
| `lib/db.ts` | Thin database client for user and project metadata (IndexedDB-based) |
| `lib/opfs.ts` | High-level OPFS API: read, write, list, and delete project files in the browser's sandboxed FS |
| `lib/opfs-crypto.ts` | Encrypts/decrypts file content before writing to / after reading from OPFS |
| `lib/opfs-write-queue.ts` | Serialises concurrent OPFS writes into a queue to prevent race conditions on shared file handles |
| `lib/preview-ping.ts` | Polls the WebContainer dev server port until it responds, then signals the preview iframe |
| `lib/projects.ts` | Project-level operations: create, open, rename, delete â€” coordinates `db.ts` and `opfs.ts` |
| `lib/react-starter-template.ts` | Returns the default in-memory file tree (minimal React + Vite project) for new projects |
| `lib/session.ts` | Manages the browser-side session token via Next.js Route Handlers |
| `lib/useWebContainer.ts` | **Central React hook** â€” boots the WebContainer singleton, runs `npm install`, starts the dev server |
| `lib/validation.ts` | Zod schemas for validating API route request bodies (signup, login, project creation) |
| `lib/wc-server-headers.ts` | Configures COOP/COEP headers required by SharedArrayBuffer (needed by WebContainers) |
| `lib/wc-sync-manager.ts` | The only authorised path to `webcontainerInstance.fs` â€” called exclusively on Keep |

### `public/`

| File | Responsibility |
|---|---|
| `public/ai-worker.js` | AI Web Worker entry point; loads WebLLM model, streams token completions back via `postMessage` |

---

## 4. Core Logic Breakdown

### 4.1 AI Model Loading State Machine

State lives in `lib/ai-engine.ts` (React Context, exposed via `useAIContext()`):

```
 "idle"
   â”‚  User clicks "Load Model"
   â–¼
 "loading"       â† WebLLM fetches model shards from CDN + compiles WASM
   â”‚              (progress % forwarded via postMessage â†’ loadProgress 0â†’1)
   â–¼
 "ready"         â† Prompt input unlocked; inference can begin
   â”‚  (on error at any stage)
   â–¼
 "error"         â† Error message displayed; retry is possible

 "generating"    â† Active during a streaming chat completion
```

Key details:
- The AI Worker is created **exactly once** inside `AIEngineProvider`'s `useEffect`. Hiding/showing `ChatSidebar` only unmounts the UI â€” the worker keeps running.
- A 2-second **heartbeat** `postMessage` is sent from the worker during generation to prevent any cleanup logic from assuming the worker stopped.
- Token chunks are accumulated into a `streamBufferRef` (no React re-renders per token). A `setInterval` flushes the buffer to `useState` every **100ms** for smooth UI updates.

### 4.2 WebContainer FS Sync (`webcontainerInstance.fs`)

`lib/useWebContainer.ts` owns the container lifecycle:

```
useWebContainer() hook mounts
  â”‚
  â”œâ”€â–º WebContainer.boot()            (one-time, guarded by a module-level ref)
  â”œâ”€â–º mount(starterTemplate)         (writes initial file tree to WC's in-memory FS)
  â”œâ”€â–º run "npm install"              (spawns inside WC, streams install logs to terminal)
  â””â”€â–º run "npm run dev"              (starts Vite dev server, captures preview URL from stdout)
```

Once running, `lib/wc-sync-manager.ts` is the **only** path that writes files back:

```typescript
webcontainerInstance.fs.writeFile(path, content, { encoding: 'utf-8' })
```

This triggers Vite HMR inside the container, which hot-reloads the preview iframe. OPFS (`lib/opfs.ts` + `lib/opfs-crypto.ts`) is a **parallel durable copy** â€” it persists encrypted files to `navigator.storage` so the project survives a page refresh. On reload, `useWebContainer` remounts the FS from OPFS rather than the blank starter template.

### 4.3 Keep / Discard â€” The Safety Gate

The Safety Gate prevents unreviewed AI changes from overwriting files. It lives in `ChatSidebar.tsx`.

```
ai-parser.ts emits FileAction[]
       â”‚
       â–¼
ChatSidebar.tsx  handleAccept()
  â€¢ Resolves patches via applyPatch()
  â€¢ Reads current file for diff context
  â€¢ Pushes to pendingChanges[] state
  â€¢ Does NOT write anything yet
       â”‚
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Keep         â”‚ Discard
  â–¼              â–¼
handleKeep()   handleDiscard()
  calls          sets actionStatus
  onApply-       â†’ "rejected"
  FileAction()   clears from
  â†’ writes to    pendingChanges
  WC fs +
  OPFS
```

**Files involved in the Safety Gate:**

| File | Role |
|---|---|
| `lib/ai-parser.ts` | **Producer** â€” emits `FileAction` objects from streamed LLM tokens |
| `app/components/editor/ChatSidebar.tsx` | **Gate owner** â€” holds `pendingChanges` state and all Keep/Discard handlers |
| `app/components/editor/DiffView.tsx` | **Gate UI** â€” renders the per-file diff and Accept/Reject buttons |
| `lib/wc-sync-manager.ts` | **Consumer** â€” called only on Keep; writes to `webcontainerInstance.fs` |
| `lib/opfs-crypto.ts` | **Persistence** â€” encrypts and durably stores the accepted change |

> âš ï¸ **Important:** `wc-sync-manager.ts` must never be called directly from `ai-engine.ts`. All writes must go through the Safety Gate in `ChatSidebar.tsx`. Bypassing it would allow partial or malformed AI output to silently corrupt the running project.

While `pendingChanges.length > 0`, the chat input is **disabled** â€” the user must resolve all pending changes before sending a new prompt.

---

## 5. Critical Workflows

### 5.1 "Load Model" Button Click Sequence

```
1.  User clicks "Load Model" in the AI panel
2.  ChatSidebar calls ai.loadModel() from useAIContext()
3.  AIEngineProvider sets status â†’ "loading", posts { type: "init" } to worker
4.  ai-worker.js receives the message, calls mlc.CreateMLCEngine(modelId, { initProgressCallback })
5.  WebLLM fetches model shards from CDN (cached to IndexedDB after first download)
6.  Worker fires postMessage({ type: "init-progress", progress, text }) per shard
7.  AIEngineProvider updates loadProgress (0â†’1) and loadText; progress bar animates
8.  When all shards are ready, WebLLM compiles them (WASM JIT)
9.  Worker fires postMessage({ type: "init-done", modelId })
10. AIEngineProvider sets status â†’ "ready", modelId stored
11. Prompt textarea and Send button become enabled
```

### 5.2 "Keep" Button Click Sequence

```
1.  User reviews the AI-generated diff in DiffView
2.  User clicks "Accept" â†’ ChatSidebar.handleAccept(msgId, action) is called
3.  handleAccept resolves any PATCH blocks via applyPatch() from ai-parser.ts
4.  handleAccept reads current file content via readFileContent() for diff context
5.  Change is pushed to pendingChanges[] â€” no file write yet
6.  DiffView shows "Applied: path" label; input is locked
7.  User clicks "Keep" in the pending changes overlay
8.  handleKeep() calls onApplyFileAction(resolvedAction)
9.  Editor page handler calls wc-sync-manager.ts â†’ webcontainerInstance.fs.writeFile()
10. Vite HMR detects the change inside the WebContainer
11. Preview iframe hot-reloads the affected module (no full page reload)
12. Simultaneously, opfs-crypto.ts encrypts and writes the file to OPFS
13. opfs-write-queue.ts serialises the write if multiple files are in-flight
14. PendingChange is removed from the queue; overlay closes
15. If no more pending changes, input is re-enabled
```

---

## 6. Developer Hints

### Top 3 Files to Read First

| Priority | File | Why |
|---|---|---|
| ðŸ¥‡ 1 | `public/ai-worker.js` | **To change AI behaviour** â€” model selection, inference parameters (`temperature`, `max_tokens`, `stop` sequences), and the streaming loop all live here. Start here if the model cuts off early or if you want to swap models. |
| ðŸ¥ˆ 2 | `lib/ai-parser.ts` | **To change how AI output is structured** â€” defines what a valid `FILE:` rewrite and `PATCH:` search-replace block look like. If you want the AI to produce new output formats, this is the file to modify. The parser contract directly determines what the Safety Gate can display. |
| ðŸ¥‰ 3 | `app/components/editor/ChatSidebar.tsx` | **To change the chat/AI UX** â€” owns streaming state, the token buffer, `pendingChanges`, and all Keep/Discard handlers. For most AI workflow changes this is your entry point. |

### Additional Tips

- **COOP/COEP headers are mandatory.** WebContainers requires `SharedArrayBuffer`, which needs `Cross-Origin-Opener-Policy: same-origin` and `Cross-Origin-Embedder-Policy: require-corp`. If the preview is blank, check `lib/wc-server-headers.ts` and `next.config.ts` first.
- **OPFS is per-origin and per-browser profile.** Data written in Chrome is not visible in Firefox. This is intentional (security boundary) but worth knowing during cross-browser testing.
- **The WebContainer boots exactly once per tab.** There is no "soft reboot" API â€” a full page reload is required to reset container state during development.
- **Model weights are cached after first download** via the browser Cache API managed by WebLLM. Subsequent "Load Model" clicks for the same model ID are near-instant.
- **The streaming buffer flushes every 100ms.** If you need lower-latency UI updates during generation, reduce the interval in `ChatSidebar.tsx` (`startStreamBuffer`). Reducing it too far (< 16ms) will cause excessive React re-renders.
